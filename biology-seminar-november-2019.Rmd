---
title: "Wibbly Wobbly Timey Wimey Stuff"
author: "Gavin Simpson • Kim Hinz • Stefano Mezzini"
date: "Department of Biology • November 29th 2019"
output:
  xaringan::moon_reader:
    css: ['default', 'https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css', 'my.css']
    lib_dir: libs
    nature:
      titleSlideClass: ['inverse','bottom','left',my-title-slide]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
      ratio: '16:9'
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
---
class: inverse

???

We meet today on Treaty 4 lands, the territories of the Cree, Saulteaux (SOH-toh), Dakota, Lakota, Nakoda, and the homeland of the Métis Nation.

Today, these lands continue to be the shared territory of many diverse peoples.

```{r setup, include=FALSE, cache=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, dev = 'svg', echo = FALSE, message = FALSE, warning = FALSE,
                      fig.height=6, fig.width = 1.777777*6)

library('ggplot2')
library('tibble')
library('tidyr')
library('dplyr')
library('mgcv')
library('forcats')
library('mvnfast')
library('purrr')
library('gganimate')
library('gratia')
library('readr')
library('cowplot')
theme_set(theme_minimal())

## constats
anim_width <- 1000
anim_height <- anim_width / 1.77777777
anim_dev <- 'png'
anim_res <- 150
```
---
class: inverse center middle subsection

# Use statistics to learn from data in presence of noise

???

One way to describe statistics is the principled process by which we learn from data in the presence of noise and uncertainty

---
class: inverse center middle subsection

# Learning from data&hellip;?

???

Why would we want to learn from data?

---
class: inverse center middle subsection

# Estimate parameters for a theoretical model

???

Lotka-Voltera models of competition between species

---
class: inverse center middle subsection

# Compare theory with observation

???

What do the data tell us?

If we want to know how theory matches with observation then we might want to see what the data can tell us without imposing too many restrictions or constraints on our statistical model

---
class: inverse center middle subsection

# Progress with little or no theory

???

We may have little or no theory to work with, so we take an empirical approach which may lead to the development of new theory

---
class: inverse
background-image: url('./resources/franki-chamaki-z4H9MYmWIMA-unsplash.jpg')
background-size: cover

# 

.footnote[
<a style="background-color:black;color:white;text-decoration:none;padding:4px 6px;font-family:-apple-system, BlinkMacSystemFont, &quot;San Francisco&quot;, &quot;Helvetica Neue&quot;, Helvetica, Ubuntu, Roboto, Noto, &quot;Segoe UI&quot;, Arial, sans-serif;font-size:12px;font-weight:bold;line-height:1.2;display:inline-block;border-radius:3px" href="https://unsplash.com/@franki?utm_medium=referral&amp;utm_campaign=photographer-credit&amp;utm_content=creditBadge" target="_blank" rel="noopener noreferrer" title="Download free do whatever you want high-resolution photos from Franki Chamaki"><span style="display:inline-block;padding:2px 3px"><svg xmlns="http://www.w3.org/2000/svg" style="height:12px;width:auto;position:relative;vertical-align:middle;top:-2px;fill:white" viewBox="0 0 32 32"><title>unsplash-logo</title><path d="M10 9V0h12v9H10zm12 5h10v18H0V14h10v9h12v-9z"></path></svg></span><span style="display:inline-block;padding:2px 3px">Franki Chamaki</span></a>
]

???

We learn from data because it can highlight our preconceptions and biases

---

# Learning from data

```{r lm-plot, fig.height=6, fig.width = 1.777777*6}
## simulate some data for a linear model
set.seed(1)
N <- 250
lm_dat <- tibble(x = runif(N), y = 0.5 + (2.1 * x) + rnorm(N, sd = 0.5))
## plot
ggplot(lm_dat, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method = 'lm') +
    theme_minimal(base_size = 20, base_family = 'Titillium')
```

???

Learning from data could be a simple as fitting a linear regression model...

---
class: inverse
background-image: url('./resources/deep-learning-turned-up-to-11.jpg')
background-size: contain

# 

???

Or as complex as fitting a sophisticated multi-layered neural network trained on huge datasets or corpora

---

# Learning involves trade-offs

.row[
.col-6[
.center[![:scale 75%](resources/low-bias-low-variance.jpg)]
]

.col-6[
.center[![:scale 75%](resources/interpretable-black-box.jpg)]
]
]

???

Learning from data involves trade offs

We can have models that fit our data well &mdash; low bias &mdash; but which are highly variable, or

We can fit models that have lower variance, but these tend to have higher bias, i.e. fit the data less well

A linear regression model is very interpretable but unless the underlying relationship is linear it will have poor fit

Deep learning may fit data incredibly well but the model is very difficult to interpret and understand

---

# Generalized Additive Models

<br />

![](resources/tradeoff-slider.png)

.references[Source: [GAMs in R by Noam Ross](https://noamross.github.io/gams-in-r-course/)]

???

GAMs are an intermediate-complexity model

* can learn from data without needing to be informed by the user
* remain interpretable because we can visualize the fitted features

---
class: inverse middle center subsection

# GAMs fit wibbly wobbly functions

---
class: inverse
background-image: url('resources/wiggly-things.png')
background-size: contain

???

Heresey, I know, but I prefer my sci-fi more Duffer Brothers than Time Lord

---

# The wibbly wobbly stuff  &mdash; splines

```{r smooth-fun-animation, results = FALSE}
f <- function(x) {
    x^11 * (10 * (1 - x))^6 + ((10 * (10 * x)^3) * (1 - x)^10)
}

draw_beta <- function(n, k, mu = 1, sigma = 1) {
    rmvn(n = n, mu = rep(mu, k), sigma = diag(rep(sigma, k)))
}

weight_basis <- function(bf, x, n = 1, k, ...) {
    beta <- draw_beta(n = n, k = k, ...)
    out <- sweep(bf, 2L, beta, '*')
    colnames(out) <- paste0('f', seq_along(beta))
    out <- as_tibble(out)
    out <- add_column(out, x = x)
    out <- pivot_longer(out, -x, names_to = 'bf', values_to = 'y')
    out
}

random_bases <- function(bf, x, draws = 10, k, ...) {
    out <- rerun(draws, weight_basis(bf, x = x, k = k, ...))
    out <- bind_rows(out)
    out <- add_column(out, draw = rep(seq_len(draws), each = length(x) * k),
                      .before = 1L)
    class(out) <- c("random_bases", class(out))
    out
}

plot.random_bases <- function(x, facet = FALSE) {
    plt <- ggplot(x, aes(x = x, y = y, colour = bf)) +
        geom_line(lwd = 1, alpha = 0.75) +
        guides(colour = FALSE)
    if (facet) {
        plt + facet_wrap(~ draw)
    }
    plt
}

normalize <- function(x) {
    rx <- range(x)
    z <- (x - rx[1]) / (rx[2] - rx[1])
    z
}

set.seed(1)
N <- 500
data <- tibble(x     = runif(N),
               ytrue = f(x),
               ycent = ytrue - mean(ytrue),
               yobs  = ycent + rnorm(N, sd = 0.5))

k <- 10
knots <- with(data, list(x = seq(min(x), max(x), length = k)))
sm <- smoothCon(s(x, k = k, bs = "cr"), data = data, knots = knots)[[1]]$X
colnames(sm) <- levs <- paste0("f", seq_len(k))
basis <- pivot_longer(cbind(sm, data), -(x:yobs), names_to = 'bf')
basis

set.seed(2)
bfuns <- random_bases(sm, data$x, draws = 20, k = k)

smooth <- bfuns %>%
    group_by(draw, x) %>%
    summarise(spline = sum(y)) %>%
    ungroup()

p1 <- ggplot(smooth) +
    geom_line(data = smooth, aes(x = x, y = spline), lwd = 1.5) +
    labs(y = 'f(x)', x = 'x') +
    theme_minimal(base_size = 14, base_family = 'Titillium')

smooth_funs <- animate(
    p1 + transition_states(draw, transition_length = 4, state_length = 2) + 
    ease_aes('cubic-in-out'),
    nframes = 200, height = anim_height, width = anim_width, res = anim_res, dev = anim_dev)

anim_save('resources/spline-anim.gif', smooth_funs)
```

.center[![](resources/spline-anim.gif)]

???

GAMs use splines to represent the non-linear relationships between covariates, here `x`, and the response variable on the `y` axis.

---

# Splines formed from basis functions

```{r basis-functions, fig.height=6, fig.width = 1.777777*6}
ggplot(basis,
       aes(x = x, y = value, colour = bf)) +
    geom_line(lwd = 2, alpha = 0.5) +
    guides(colour = FALSE) +
    labs(x = 'x', y = 'b(x)') +
    theme_minimal(base_size = 20, base_family = 'Titillium')
```

???

Splines are built up from basis functions

Here I'm showing a cubic regression spline basis with 10 knots/functions

We weight each basis function to get a spline. Here all the basisi functions have the same weight so they would fit a horizontal line

---

# Weight basis functions &rarr; spline

```{r basis-function-animation}
bfun_plt <- plot(bfuns) +
    geom_line(data = smooth, aes(x = x, y = spline),
              inherit.aes = FALSE, lwd = 1.5) +
    labs(x = 'x', y = 'f(x)') +
    theme_minimal(base_size = 14, base_family = 'Titillium')

bfun_anim <- animate(
    bfun_plt + transition_states(draw, transition_length = 4, state_length = 2) + 
    ease_aes('cubic-in-out'),
    nframes = 200, height = anim_height, width = anim_width, res = anim_res, dev = anim_dev)

anim_save('resources/basis-fun-anim.gif', bfun_anim)
```

.center[![](resources/basis-fun-anim.gif)]

???

But if we choose different weights we get more wiggly spline

Each of the splines I showed you earlier are all generated from the same basis functions but using different weights

---

# How do GAMs learn from data?

```{r example-data-figure, fig.height=6, fig.width = 1.777777*6}
data_plt <- ggplot(data, aes(x = x, y = ycent)) +
    geom_line(col = 'goldenrod', lwd = 2) +
    geom_point(aes(y = yobs), alpha = 0.2, size = 3) +
    labs(x = 'x', y = 'f(x)') +
    theme_minimal(base_size = 20, base_family = 'Titillium')
data_plt
```

???

How does this help us learn from data?

Here I'm showing a simulated data set, where the data are drawn from the orange functions, with noise. We want to learn the orange function from the data

---

# Choose weights to best fit data

```{r basis-functions-anim}
sm2 <- smoothCon(s(x, k = k, bs = "cr"), data = data, knots = knots)[[1]]$X
beta <- coef(lm(ycent ~ sm2 - 1, data = data))
wtbasis <- sweep(sm2, 2L, beta, FUN = "*")
colnames(wtbasis) <- colnames(sm2) <- paste0("F", seq_len(k))
## create stacked unweighted and weighted basis
basis <- as_tibble(rbind(sm2, wtbasis)) %>%
    add_column(x = rep(data$x, times = 2),
               type = rep(c('unweighted', 'weighted'), each = nrow(sm2)),
               .before = 1L)
##data <- cbind(data, fitted = rowSums(scbasis))
wtbasis <- as_tibble(rbind(sm2, wtbasis)) %>%
    add_column(x      = rep(data$x, times = 2),
               fitted = rowSums(.),
               type   = rep(c('unweighted', 'weighted'), each = nrow(sm2))) %>%
    pivot_longer(-(x:type), names_to = 'bf')
basis <- pivot_longer(basis, -(x:type), names_to = 'bf')

p3 <- ggplot(data, aes(x = x, y = ycent)) +
    geom_point(aes(y = yobs), alpha = 0.2) +
    geom_line(data = basis,
              mapping = aes(x = x, y = value, colour = bf),
              lwd = 1, alpha = 0.5) +
    geom_line(data = wtbasis,
              mapping = aes(x = x, y = fitted), lwd = 1, colour = 'black', alpha = 0.75) +
    guides(colour = FALSE) +
    labs(y = 'f(x)', x = 'x') +
    theme_minimal(base_size = 14, base_family = 'Titillium')

crs_fit <- animate(p3 + transition_states(type, transition_length = 4, state_length = 2) + 
                   ease_aes('cubic-in-out'),
                   nframes = 100, height = anim_height, width = anim_width, res = anim_res,
                   dev = anim_dev)

anim_save('./resources/gam-crs-animation.gif', crs_fit)
```

.center[![](resources/gam-crs-animation.gif)]

???

Fitting a GAM involves finding the weights for the basis functions that produce a spline that fits the data best, subject to some constraints

---
class: inverse middle center subsection

# Avoid overfitting our sample

---
class: inverse middle center subsection

# Use a wiggliness penalty &mdash; avoid fitting too wibbly wobbly models

---

# Outputs

* Developing methodological approaches

* Developing packages to enable model-fitting by other scientists

* Training

.row[

.col-6[
![](resources/frontiers-paper-title.png)
.small[
Simpson (2018) *Frontiers in Ecology & Evolution*

[doi: 10/gfrc4p](http://doi.org/gfrc4p)
]
]

.col-6[
![](resources/hgam-paper-title.png)
.small[
Pedersen *et al* (2019) *PeerJ*

[doi: 10/c6wz](http://doi.org/c6wz)
]
]

]

---
class: inverse middle center subsection

# Trends in water quality data

---
class: inverse
background-image: url('resources/memes-are-cool.jpg')
background-size: contain

# 

???

Memes are cool

---

# Why statistics?

* Estimate effect size

* Estimate variance

* Make predictions from a model

--

.bigger[.center[*Stats are cool*™]]

---

# Badly explained stats

.row[

.col-6[
.big[Stats profs be like]

.centre[![:scale 120%](resources/stats-are-cool.jpg)]
]

.col-6[]

]

---

# Badly explained stats

.row[

.col-6[
.big[Stats profs be like]

.centre[![:scale 120%](resources/stats-are-cool.jpg)]
]

.col-6[
.big[Students be like]

.centre[![:scale 83%](resources/amy-pond-sure.jpg)]
]

]

---

# Badly explained stats

GAMs

1. Conditional distribution

2. &nbsp;

3. &nbsp;

.bigger[
$$y_i \sim EF(\mu_i, \Theta)$$
]

---
class: inverse
background-image: url(resources/wait-what-dw0.jpg)
background-size: cover

# 

---
# Badly explained stats

GAMs

1. Conditional distribution

2. Link function

3. &nbsp;

.big[
$$g(\mathbb{E}(y_i)) = g(\mu_i) = \eta_i$$
]

---
class: inverse
background-image: url(resources/wait-what-dw1.jpg)
background-size: contain

# 

---

# Badly explained stats

GAMs

1. Conditional distribution

2. Link function

3. Linear predictor

.big[
$$\eta = \beta_0 + \sum_{j=1}^{p} f_j(x_j)$$
]

---
class: inverse
background-image: url(resources/wait-what-dw2.jpg)
background-size: cover

# 

---

# Badly explained stats

GAMs

1. Conditional distribution

2. Link function

3. Linear predictor

*Homework*: read chapter on GAMs and do ex. 1&ndash;20

---
class: inverse

# 

<video controls>
  <source src="resources/timey-wimey-trends-subbed.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>

---
class: inverse middle center subsection

# Let's skip the theory and see the applications!

---

# Master Agreement on Apportionment

.row[

.col-4[

* 12 rivers in AB, SK, MB

* Apportionment of inter-provincial river water

* Water quality objectives

]

.col-8[
.center[![](resources/maa-map.gif)]
]

]

---
class: inverse middle center subsection

# Sulphate in the Assiniboine River

---

# Sulphate in the Assiniboine River

Sulphate ([SO<sub>4</sub>]) in Assiniboine River near Shellmouth

* No significant increases over the years

* Keep [SO<sub>4</sub>] < 299 mg L<sup>-1</sup>

.row[

.col-12[
.center[![:scale 90%](resources/assiniboine-at-shellmouth.png)]
]

]

---
background-image: url('resources/so4.png')
background-size: contain

# The data

---

# The seasonal Mann-Kendall test

* Test statistic used for the detection of a trend in a time series

* Commonly used for water quality assessment (Hirsch et al. 1982)

* Used by the Prairie Provinces Water Board &mdash; responsible for MAA

* Assumes that the trend is monotonic

.references[
Hirsch, R.M., Slack, J.R. and Smith, R.A. (1982), Techniques for trend assessment for monthly water quality data, *Water Resources Research* **18**, 107&ndash;121.
]

---
class: inverse middle center subsection

# GAMs should provide a more robust approach to trend detection

---

# Model structure I

Create a model with *year*, *seasonal*, and *flow* effects

--
.row[

.col-12[
.center[![](resources/so4-smooths.png)]
]

]

---

# Model structure II

Add interactions between terms

--
.row[

.col-12[
.center[![](resources/so4-tis.png)]
]

]

???

How seasonal patterns change over the years,
How the effect of flow changes over the years (e.g. FX of dilution in seasons of high/low pollution),
How the effect of flow changes over the seasons (e.g. FX of dilution in years of high/low pollution)

---

# Using the model

1. Estimate $\mathbb{E}([\text{SO}_4])$ over time

2. Estimate probability [SO<sub>4</sub>] exceeds the guideline (299 mg L<sup>-1</sup>) over time

3. Identify years of significant increase in [SO<sub>4</sub>]

---

# Expected [SO<sub>4</sub>] over time

<br />

.row[

.col-12[
.center[![](resources/wibbly-wobbly-so4.gif)]
]

]

---

# Expected [SO<sub>4</sub>] given log(Flow)

.row[

.col-12[
.center[![](resources/so4-rayshader.png)]
]

]

---

# P( [SO<sub>4</sub>] &gt;299 mg L<sup>-1</sup> ) given log(Flow)

.row[

.col-12[
.center[![](resources/so4-prob.png)]
]

]

---

# Instantaneous rate of change

<br />
.row[

.col-12[
.center[![](resources/so4-derivative.png)]
]

]

---

# Identify periods of change

<br />
.row[

.col-12[
.center[![](resources/so4-derivative.gif)]
]

]

---
class: inverse
background-image: url('resources/p-hacking.jpg')
background-size: contain
background-position: right

# using α = 0.05 <br/ >as a guide&hellip;

---

# Identify periods of significant change

<br />
.row[

.col-12[
.center[![](resources/so4-derivative-signif.gif)]
]

]

---

# Conclusions

Using the model we can show that:

1. The expected value of [SO<sub>4</sub>] was often &gt; 299 mg L<sup>-1</sup> &mdash; esp. after 2010

2. The probability that [SO<sub>4</sub>] exceeded 299 mg L<sup>-1</sup> was often high &mdash; esp. after 2010

3. [SO<sub>4</sub>] increased markedly post 2008 &mdash; **failure of the water quality objectives**

<br />

.big[[SO<sub>4</sub>] in the Assiniboine River should be monitored more closely]

---
class: inverse middle center subsection

# Saskatchewan's changing climate

---
background-image: url('resources/station-map.gif')
background-size: contain
background-position: right

# Saskatchewan's<br /> temperature

.row[

.col-7[

* Adjusted and Homogenized Canadian Climate Data (AHCCD)

* Monthly mean temperature

* 36 climate stations

* Variables

    * Year
	* Month
	* Latitude
	* Longitude
	* Climate Station

* 36693 observations

]

.col-5[

]
]

???

~ 36 stations ranging from Uranium City in the Taiga Shield to Poplar River near the Canadian-US border

Random effect allows the model to account for the variance between locations due to any systematic/random error

There 36 693 observations

Daily temperature data are available, and models have been run for these, but we decided to use monthly temperature because there were less than 37 thousand observations vs the 1.1 million

Mention map

---
class: inverse middle center subsection

# yes&hellip; moar GAMs&hellip;

???

To show changing trends, I used HGAM…

After Gavin’s and Stefano’s presentations, expect one of the following thoughts on GAMs.

---
class: inverse
background-image: url('resources/understand.png')
background-size: contain

---
class: inverse
background-image: url('resources/sure.png')
background-size: contain

---
class: inverse
background-image: url('resources/lost-focus.png')
background-size: contain

---
class: inverse
background-image: url('resources/linear-vs-gam.png')
background-size: contain

???

At the very simplest, GAMs better model wiggly data and show trends more accurately than linear models would

---
class: inverse middle center subsection

# Hierarchical GAMs

???

Don’t freak out before I get the chance to present on why I’m really here

HGAMs are a lot simpler than you may expect

Only difference is data can be grouped, and trends can vary between the groups

---
class: inverse
background-image: url('resources/confused-fear.png')
background-size: contain

---
class: inverse
background-image: url('resources/confused-again.png')
background-size: contain

---

# Hierarchical GAMs

* HGAMs are similar to GAMs

* Instead of one model per time series, model all time series at once

* Smooths can vary between time series

* Can determine

    * common trend over all stations
	
	* unique trend for each station

---
class: inverse
background-image: url('resources/simple.png')
background-size: contain

???

Quick heads up

---
class: inverse
background-image: url('resources/i-dont-know.gif')
background-size: contain

--

.footnote[The *Stavrinauts* made me add this&hellip;]

---
class: inverse middle center subsection

# Why a hierarchical GAM?

???

Not to bore with stats jargon

---
class: inverse middle center subsection

# Many studies of Canadian temperature & climate data

---
class: inverse middle center subsection

# Individual linear models for each climate station

---
class: inverse middle center subsection

# Have to compare all the models *post hoc*

---
class: inverse middle center subsection

# We know temperature not changing linearly

---
class: inverse middle center subsection

# HGAMs allow us to model wiggly curves and account for spatial trends

???

HGAMs allow us to model wiggly curves to the data and account for location differences, all in one model

---
class: inverse
background-image: url('resources/one-model.png')
background-size: contain

---
background-image: url('resources/draw-time.png')
background-size: 95%

# Effects of time

???

Break down components of the model

Left: average seasonal trend across all stations, summers much warmer than winters

---
class: inverse
background-image: url('resources/no-kidding.png')
background-size: contain

---
background-image: url('resources/draw-time.png')
background-size: 95%

# Effects of time

???

Difference is 30-40 degrees

Middle: temperature change throughout the years

Fewer temperature stations in the beginning, hence the large Cis

Temperature has become more variable between years

Right: while left shows average trends, the right plot shows how seasonal trends have changed by year

---
background-image: url('resources/month-years-main.gif')
background-size: 85%
background-position: bottom

# Seasonal temperature changes over time

???

Three main cities in Saskatchewan for last 118 years

Temperatures are increasing throughout the seasons, but the winters are more drastic

---
background-image: url('resources/draw-lat-long.png')
background-size: contain
background-position: right

# Effect of space

.row[

.col-6[
* Northern SK is colder than southern SK
]

.col-6[
]

]

???

First thing to notice is that north SK is colder than south SK

---
class: inverse
background-image: url('resources/no-kidding.png')
background-size: contain

---
background-image: url('resources/draw-lat-long.png')
background-size: contain
background-position: right

# Effect of space

.row[

.col-7[
* Northern SK is colder than southern SK

* Difference is about 9&#8451;

* Fewer stations in northern SK

* Need to use the model to extrapolate between stations
]

.col-5[

]

]

???

9 degrees difference

Fewer stations in north; therefore, use model to extrapolate for all possible

---
background-image: url('resources/lat-long.gif')
background-size: contain
background-position: right

# Space & time

.row[

.col-7[
* 2018 modelled temperature

* Spatial pattern changes throughout the year

* Greater temperature variability in northern SK
]

.col-5[

]

]

???

How spatial trends change throughout the year

North SK has these two vertical bands that are warmer in the summer and colder in the winter than the adjacent area

South SK is not as variable and remains even throughout the year

---
background-image: url('resources/draw-time.png')
background-size: 95%

# Effects of time

???

Refresher, these plots show the global/average trend, but we also want to know how individual stations vary around these average trends

---
background-image: url('resources/draw-location.png')
background-size: 95%
background-position: bottom

# Station effects

???

First plot shows how locations vary within years

Some have warmer winters and cooler summers than average

Some are the inverse. Most stay around the average

Second plot: how locations vary throughout the years

Again, most stay around the average. But some have increased

Two weird ones that have cooled and are now closer to the average

---

# Conclusions

* HGAMs are useful for modelling wiggly data from many climate stations

* Temperatures have significantly increased across Saskatchewan since the 1880s

* This change is more clearly seen in the winter months

* Seasonal trends vary spatially and temporally

* Significant variation in the trends at each climate station

---
class: inverse middle center subsection

# Climate change affecting lake temperatures?

---
class: inverse
background-image: url('resources/fellow-kids-vsn-1.gif')
background-size: contain

---

```{r load-process-lake-temp-data}
## load GEV support functions
source("./functions/gev-distribution.R")

## load data
lake <- read_csv("./data/lake-temperature-time-series.csv",
                 skip = 1L,         # setting col name so skip those provided
                 col_names = c('date','temperature','lake'),
                 col_types = 'Ddc') # D = Date, d = double, c = character

## prepare data
## add some useful variables
lake <- mutate(lake,
               year  = as.numeric(format(date, format = '%Y')),
               lake  = factor(lake),
               lake  = fct_recode(lake,
                                  `Blelham Tarn` = 'BLEL',
                                  `Esthwaite Water` = 'ESTH',
                                  Feeagh = 'FEEAGH',
                                  `Loch Leven` = 'LEVEN',
                                  `Vättern` = 'VATTERN',
                                  `Windermere North` = 'NBAS',
                                  `Windermere South` = 'SBAS',
                                  `Wörthersee` = 'WORTHERSEE',
                                  `Zürichsee` = 'ZURICH'))

## compute annual minimum lake water temperature
minima <- lake %>% group_by(lake, year) %>%
    summarize(minimum = min(temperature, na.rm = TRUE)) %>%
    mutate(cyear = year - median(seq(min(year), max(year), by = 1L)),
           neg_min = -minimum) %>% 
    ungroup()

## store the median year that we used to centre year by
med_year <- minima %>% select(year) %>%
    summarise(median_year = median(year)) %>%
    pull(median_year)

## constants for plots
min_temp_label <- 'Surface temperature (°C)'

## model set up
ctrl <- gam.control(nthreads = 4) # use multiple threads to fit

## model
m1 <- gam(list(neg_min ~ s(cyear, lake, bs = 'fs'),
               ~ s(cyear, lake, bs = 'fs'),
               ~ s(cyear, lake, bs = 'fs')),
          data = minima, method = 'REML',
          family = gevlss(link = list('identity', 'identity', 'logit')),
          control = ctrl, optimizer = 'efs')

fit <- fitted(m1)
mu  <- fit[,1]
rho <- fit[,2]
xi  <- fit[,3]
fv  <- mu + exp(rho) * (gamma(1 - xi) - 1) / xi

## Get the predicted expected response...
m1_diag <- tibble(fitted = -(mu + exp(rho) * (gamma(1 - xi) - 1) / xi),
                  resid  = residuals(m1))
m1_diag <- bind_cols(minima, m1_diag)

## the next block of code simulates ~95% confidence interval
## on the fitted values only, and plots this with the observed
## data
sims <- fitted_draws_gev_gam(m1, nsim = 10000, seed = 3476,
                             minima = TRUE, ncores = 4)
upper <- apply(sims, 1L, quantile, probs = 0.975)
lower <- apply(sims, 1L, quantile, probs = 0.025)

m1_diag <- add_column(m1_diag, lower = lower, upper = upper)

## repeat this but for N values spread evenly over time series
## of each lake
N <- 200

## this requires a function to be applied to the data for each lake
## this simply returns a tibble with `lake`, `year`, `cyear`
## with `year` being N values spread evenly from the first year to
## the last
year_seq <- function(tbl, n, median_year) {
    df <- tibble(lake = rep(unique(tbl$lake), n),
                 year = seq_min_max(tbl$year, n = n))
    mutate(df, cyear = year - median_year)
}

## do the split-apply-combine to get year sequences per lake
newd <- minima %>% group_by(lake) %>%
    split(.$lake) %>%
    map_dfr(year_seq, n = N, median_year = med_year)

## add expected values from esimtated GEV on to the prediction data
m1_pred <- mutate(newd, fitted = -expected_gev(m1, newdata = newd))

## simulate 10000 draws from posterior of the model and return expected
## values from the GEV for each posterior draw
sims <- fitted_draws_gev_gam(m1, newdata = newd, nsim = 10000, seed = 3476,
                             minima = TRUE, ncores = 4)
## compute ~ 95% confidence intervals
upper <- apply(sims, 1L, quantile, probs = 0.975)
lower <- apply(sims, 1L, quantile, probs = 0.025)
## ...and add these to the prediction data
m1_pred <- add_column(m1_pred, lower = lower, upper = upper)

## Next blocks of code will compute the density of the estimated GEV
## distributions for common start years and end years for each lake
##
## We use the earliest and latest years for which we have data for all lakes
start_year <- minima %>% group_by(lake) %>%
    summarise(min_year = min(year)) %>%
    ungroup() %>%
    pull(min_year) %>%
    max()
## ...
last_year <- minima %>% group_by(lake) %>%
    summarise(max_year = max(year)) %>%
    ungroup() %>%
    pull(max_year) %>%
    min()

## Data to predict at; expansion of the start and end years and the 9 lakes
newd <- crossing(year = c(start_year, last_year),
                 lake = levels(minima$lake)) %>%
    mutate(cyear = year - med_year)

## predict GEV parameters for each lake in the two years
pred <- predict(m1, newdata = newd, type = 'response')
colnames(pred) <- c('mu','sigma','xi')
## need to put these on the correct scales
##  mu:    negated because this is block maxima model and so we modelled
##           the negative of the minima (exactly equivalent)
##  sigma: gevlss() sigma parameterised in terms of log(sigma) == rho
##           so we use the inverse link to back-transform
## probably should do this properly using the inverse link functions
##   in here('functions') in case we change the links in the model...
pred <- as_tibble(pred) %>%
    mutate(mu = -mu, sigma = exp(sigma))

## Need all combinations of GEV parameters and the sequence of
## minimum temperatures to evaluate density over (`x`)
gev_pars <- crossing(pred, x = seq(-1.5, 8, length.out = 1000))

## stick prediction data and predicted GEV parameters together
## Note: can't do this before we create `gev_pars()` via `crossing()`
##         as we don't want all combinations of *all* variables
pred <- bind_cols(newd, pred)

## compute the GEV density for all lakes & prepare data for plotting
gev_dens <- left_join(gev_pars, pred, by = c('mu', 'sigma', 'xi')) %>%
    select(-cyear) %>% 
    mutate(density = dgev(x, mu, sigma, xi),
           year = factor(year))
```

# 

.references[
Data: Woolway *et al* (2019) *Climate Change* **155**, 81&ndash;94 [doi: 10/c7z9](http://doi.org/c7z9)
]

```{r plot-blelham}
ggplot(filter(lake, lake == 'Blelham Tarn'), aes(x = date, y = temperature)) +
    geom_line() +
    guides(colour = FALSE) +
    theme_minimal(base_size = 20, base_family = 'Titillium') +
    labs(x = NULL, y = min_temp_label, title = 'Blelham Tarn, UK')
```

---
class: inverse middle center subsection

# Why worry about minimum temperatures?

---

# Why worry about minimum temperatures?

Annual minimum temperature is a strong control on many in-lake processes (eg Hampton *et al* 2017)

Extreme events can have long-lasting effects on lake ecology &mdash; mild winter in Europe 2006&ndash;7 (eg Straile *et al* 2010)

Reduction in habitat or refugia for cold-adapted species

* Arctic charr (*Salvelinus alpinus*)
* Opossum shrimp (*Mysis salemaai*)

.references[Hampton *et al* (2017). Ecology under lake ice. *Ecology Letters* **20**, 98–111. [doi: 10/f3tpzh](http://doi.org/f3tpzh)

Straile *et al* (2010). Effects of a half a millennium winter on a deep lake &mdash; a shape of things to come? *Global Change Biology* **16**, 2844–2856. [doi: 10/bx6t4d](http://doi.org/bx6t4d)]

---
class: inverse
background-image: url('./resources/colbert-extremes.gif')
background-size: contain

# 

---

# Multiple time series &rarr; HGAM

```{r plot-all-lakes}
ggplot(lake, aes(x = date, y = temperature, colour = lake)) +
    geom_line() +
    facet_wrap(~ lake) +
    guides(colour = FALSE) +
    theme_minimal(base_size = 20, base_family = 'Titillium') +
    labs(x = NULL, y = min_temp_label)
```

---
class: inverse
background-image: url('./resources/one-does-not-simply.jpg')
background-size: contain

# 

---
class: inverse center middle subsection

# central limit theorem

???

Central limit theorem shows us that the Gaussian or normal distribution is the sampling distribution for many sample statistics, including sample means, as samples sizes become large

Central limit theorem underlies much of the theory that justifies much of the statistics you learn about in your statistics courses, and supports the use of the Gaussian or normal distribution

---

# Annual minimum temperature

```{r plot-minima}
ggplot(minima, aes(x = year, y = minimum, colour = lake)) +
    geom_line(lwd = 1) +
    facet_wrap(~ lake) +
    guides(colour = FALSE) +
    theme_minimal(base_size = 20, base_family = 'Titillium') +
    labs(x = NULL, y = min_temp_label)
```

---
class: inverse middle center subsection

# block minima

---

# Fisher&ndash;Tippett&ndash;Gnedenko theorem

> The **maximum** of a sample of iid random variables after proper renormalization can only converge in distribution to one of three possible distributions; the *Gumbel* distribution, the *Fréchet* distribution, or the *Weibull* distribution.

.row[

.col-4[
.center[![:scale 95%](./resources/fisher.jpg)
.smaller[Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:%E0%A4%B0%E0%A5%8B%E0%A4%A8%E0%A4%BE%E0%A4%A1%E0%A5%8D.jpg)]]
]
.col-4[ 
.center[![:scale 88%](./resources/tippett.jpg)]
.smaller[Source: [ral.ucar.edu](https://ral.ucar.edu/projects/extremes/Extremes/history.html)]
]
.col-4[ 
.center[![:scale 97%](./resources/gnedenko.png)]
.smaller[Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Gnedenko.png)]
]

]

---
class: inverse middle center subsection

# block minima&hellip;?

---
class: inverse middle center subsection

# highly technical fix

---

# Negate the minima

```{r plot-minima-negated}
ggplot(minima, aes(x = year, y = -minimum, colour = lake)) +
    geom_line(lwd = 1) +
    facet_wrap(~ lake) +
    guides(colour = FALSE) +
    theme_minimal(base_size = 20, base_family = 'Titillium') +
    labs(x = NULL, y = min_temp_label)
```

---
class: inverse middle center subsection

# plus some jiggery-pokery after model fitting

---
class: inverse middle center subsection
background-image: url('resources/the-scream.jpg')
background-size: cover

# three distributions &mdash; WTF

---

# Generalised extreme value distribution

In 1978 Daniel McFadden demonstrated the common functional form for all three distributions — the GEVD

$$G(y) = \exp \left \{ - \left [ 1 + \xi \left ( \frac{y - \mu}{\sigma} \right ) \right ]_{+}^{-1/\xi} \right \}$$

.row[
.col-6[
Three parameters to estimate

* location $\mu$,
* scale $\sigma$, and
* shape $\xi$
]
.col-6[
Three distributions

* Gumbel distribution when $\xi$ = 0,
* Fréchet distribution when $\xi$ > 0, and
* Weibull distribution when $\xi$ < 0
]
]

---
class: inverse middle center subsection

# Fit HGAMLSS using GEV for response

---
class: inverse middle center subsection

# HGAMLSS&hellip;?

---
class: inverse middle center subsection

# Model μ, σ, ξ with smooths of Year

---

# Estimated smooths

&nbsp;

```{r draw-gev-model-smooths, fig.height=4.5}
mu_smooths <- evaluate_smooth(m1, 's(cyear,lake)')
si_smooths <- evaluate_smooth(m1, 's.1(cyear,lake)')
xi_smooths <- evaluate_smooth(m1, 's.2(cyear,lake)')
mu_plt <- ggplot(mu_smooths, aes(x = cyear, y = est, colour = lake)) +
    geom_line() +
    guides(colour = FALSE) +
    theme_minimal(base_size = 20, base_family = 'Titillium') +
    labs(x = 'Year (centred)', y = 'Effect', title = expression(mu ~ '~ s(cyear,lake)'))
si_plt <- ggplot(si_smooths, aes(x = cyear, y = est, colour = lake)) +
    geom_line() +
    guides(colour = FALSE) +
    theme_minimal(base_size = 20, base_family = 'Titillium') +
    labs(x = 'Year (centred)', y = 'Effect', title = expression(sigma ~ '~ s(cyear,lake)'))
xi_plt <- ggplot(xi_smooths, aes(x = cyear, y = est, colour = lake)) +
    geom_line() +
    guides(colour = FALSE) +
    theme_minimal(base_size = 20, base_family = 'Titillium') +
    labs(x = 'Year (centred)', y = 'Effect', title = expression(xi ~ '~ s(cyear,lake)'))

plot_grid(mu_plt, si_plt, xi_plt, ncol = 3, align = 'h', axis = 'tb')
```

---

# 

```{r fitted-values-plot}
## plot of fitted vs observed minimum faceted by lake
ggplot(m1_diag, aes(x = fitted, y = minimum, colour = lake)) +
    geom_abline(slope = 1, intercept = 0, lwd = 1) +
    geom_point(size = 2.5, alpha = 0.6) +
    facet_wrap(~ lake) +
    guides(colour = FALSE) +
    theme_minimal(base_size = 20, base_family = 'Titillium') +
    labs(x = 'Model estimated temperature (°C)', y = min_temp_label)
```

---

# 

```{r fitted-trends-plot}
ggplot(minima, aes(x = year, y = minimum, colour = lake)) +
    geom_line(lwd = 1) +
    geom_ribbon(aes(ymin = lower, ymax = upper, x = year), data = m1_pred,
                alpha = 0.3, inherit.aes = FALSE) +
    geom_line(aes(y = fitted, x = year), data = m1_pred, lwd = 1, inherit.aes = FALSE) +
    facet_wrap(~ lake) +
    labs(x = NULL, y = min_temp_label) +
    theme_minimal(base_size = 20, base_family = 'Titillium') +
    guides(colour = FALSE)
```

---

# Summary

* Lake **minimum** surface water temperatures have increased by on the order of 1&ndash;3 degrees over the last 60 years

* Evidence that the distribution of annual minima has changed in many lakes &mdash; implications for future extreme events which have long-term knock-on effects

* HGAMLSS with the GEV distribution are a good way of modelling common trends in environmental extremes

---

# Acknowledgements

### Funding

.row[

.col-6[
.center[![:scale 70%](./resources/NSERC_C.svg)]
]

.col-6[
.center[![:scale 70%](./resources/fgsr-logo.jpg)]
]

]

### Data

* Prairie Provinces Water Board &mdash; Dr. Joanne Sketchell
* Environment and Climate Change Canada & Government of Canada
* Iestyn Woolway and colleagues for archiving the lake surface water data

### Slides

* HTML Slide deck [bit.ly/bio-wibbly](http://bit.ly/bio-wibbly) &copy; Simpson, Hinz, Mezzini (2019) [![Creative Commons Licence](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)
* RMarkdown [Source](https://github.com/simpson-lab/biology-seminar-2019)
